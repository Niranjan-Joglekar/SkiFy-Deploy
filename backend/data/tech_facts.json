[
  {
    "id": "py_gil",
    "topic": "Python",
    "content": "The Global Interpreter Lock (GIL) is a mutex that prevents multiple native threads from executing Python bytecodes simultaneously. This makes Python threads ineffective for CPU-bound tasks (like image processing) but safe for I/O-bound tasks (like network requests)."
  },
  {
    "id": "py_decorator",
    "topic": "Python",
    "content": "A Decorator is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. It is usually implemented as a function that takes another function as an argument and returns a new wrapper function."
  },
  {
    "id": "py_generator",
    "topic": "Python",
    "content": "Generators are functions that return an iterable set of items, one at a time, in a special way. They use the 'yield' keyword instead of 'return'. Unlike lists, they do not store all values in memory, making them highly memory-efficient for large datasets."
  },
  {
    "id": "py_memory",
    "topic": "Python",
    "content": "Python uses a private heap space to manage memory. Memory management is handled by the Python memory manager and Reference Counting. When an object's reference count drops to zero, the Garbage Collector deallocates the memory."
  },
  {
    "id": "py_context_manager",
    "topic": "Python",
    "content": "Context Managers (using the 'with' statement) allow you to allocate and release resources precisely. The most common usage is opening files. It ensures that the file is automatically closed after the block is executed, even if an exception occurs."
  },
  {
    "id": "py_mutable",
    "topic": "Python",
    "content": "In Python, arguments are passed by 'assignment'. If you pass a mutable object (like a list) to a function and modify it inside, the change persists outside. If you reassign the reference entirely inside the function, the outer variable remains unchanged."
  },
  {
    "id": "py_lambda",
    "topic": "Python",
    "content": "Lambda functions are anonymous, small functions defined with the 'lambda' keyword. They can take any number of arguments but can only have one expression. They are often used with functions like map(), filter(), and sorted()."
  },
  {
    "id": "py_deepcopy",
    "topic": "Python",
    "content": "A Shallow Copy creates a new object but inserts references into it to the objects found in the original. A Deep Copy creates a new object and recursively adds copies of the nested objects found in the original, preventing shared reference issues."
  },
  {
    "id": "py_metaclass",
    "topic": "Python",
    "content": "A Metaclass is a 'class of a class'. Just as a class defines how an instance behaves, a metaclass defines how a class behaves. In Python, type is the default metaclass for all classes."
  },
  {
    "id": "py_slots",
    "topic": "Python",
    "content": "The __slots__ magic method allows you to explicitly declare data members (attributes), preventing the creation of the default __dict__ for each instance. This saves memory when creating thousands of small objects."
  },
  {
    "id": "py_1",
    "topic": "Python",
    "content": "The Global Interpreter Lock (GIL) is a mutex that prevents multiple native threads from executing Python bytecodes simultaneously. This makes Python threads ineffective for CPU-bound tasks (like image processing) but safe for I/O-bound tasks."
  },
  {
    "id": "py_2",
    "topic": "Python",
    "content": "Python's list comprehensions are generally faster than for-loops because they are optimized at the C language level to avoid the overhead of the Python interpreter's loop control steps."
  },
  {
    "id": "py_3",
    "topic": "Python",
    "content": "Decorators are functions that take another function as an argument and extend its behavior without explicitly modifying it. They are often used for logging, access control, and instrumentation."
  },
  {
    "id": "ml_1",
    "topic": "Machine Learning",
    "content": "The Bias-Variance Tradeoff describes the tension between a model's ability to minimize bias (error from erroneous assumptions) and variance (sensitivity to small fluctuations in the training set). High bias causes underfitting; high variance causes overfitting."
  },
  {
    "id": "ml_2",
    "topic": "Machine Learning",
    "content": "Overfitting occurs when a model learns the training data (including noise) too well, negatively impacting performance on new data. Techniques to prevent it include Cross-Validation, Regularization (L1/L2), and Pruning."
  },
  {
    "id": "ml_3",
    "topic": "Machine Learning",
    "content": "Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall (Sensitivity) is the ratio of correctly predicted positive observations to the all observations in the actual class."
  },
  {
    "id": "ml_4",
    "topic": "Machine Learning",
    "content": "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. L1 Regularization (Lasso) adds the absolute value of magnitude of coefficients, while L2 (Ridge) adds the squared magnitude."
  },
  {
    "id": "ml_5",
    "topic": "Machine Learning",
    "content": "Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. The Learning Rate determines the size of the steps."
  },
  {
    "id": "ml_6",
    "topic": "Machine Learning",
    "content": "The ROC Curve (Receiver Operating Characteristic) plots the True Positive Rate against the False Positive Rate at various threshold settings. The AUC (Area Under Curve) represents the degree or measure of separability."
  },
  {
    "id": "ml_7",
    "topic": "Machine Learning",
    "content": "Bagging (Bootstrap Aggregating) involves building multiple models (typically of the same type) from different subsamples of the training dataset. Random Forest is a prime example of Bagging."
  },
  {
    "id": "ml_8",
    "topic": "Machine Learning",
    "content": "Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. It is an iterative process where each subsequent model attempts to correct the errors of the previous model (e.g., XGBoost, AdaBoost)."
  },
  {
    "id": "ml_9",
    "topic": "Machine Learning",
    "content": "K-Means Clustering is an unsupervised learning algorithm that partitions n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid)."
  },
  {
    "id": "ml_10",
    "topic": "Machine Learning",
    "content": "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information in the large set, using orthogonal linear transformations."
  },
  {
    "id": "ml_11",
    "topic": "Machine Learning",
    "content": "A Confusion Matrix is a table often used to describe the performance of a classification model. It displays True Positives, True Negatives, False Positives, and False Negatives."
  },
  {
    "id": "ml_12",
    "topic": "Machine Learning",
    "content": "Support Vector Machines (SVM) are supervised learning models used for classification and regression. They work by finding the hyperplane that best divides a dataset into classes."
  },
  {
    "id": "ml_13",
    "topic": "Machine Learning",
    "content": "The F1 Score is the harmonic mean of Precision and Recall. It is useful when you need to take both False Positives and False Negatives into account, especially if class distribution is uneven."
  },
  {
    "id": "ml_14",
    "topic": "Machine Learning",
    "content": "Cross-Validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The most common method is k-fold cross-validation."
  },
  {
    "id": "ml_15",
    "topic": "Machine Learning",
    "content": "Activation Functions (like ReLU, Sigmoid, Tanh) determine the output of a neural network node given an input or set of inputs. They introduce non-linearity, allowing the network to learn complex patterns."
  },
  {
    "id": "rag_1",
    "topic": "GenAI",
    "content": "Retrieval-Augmented Generation (RAG) is a technique that optimizes the output of an LLM so it references an authoritative knowledge base outside its training data sources before generating a response."
  },
  {
    "id": "rag_2",
    "topic": "GenAI",
    "content": "Hallucinations in LLMs occur when the model generates incorrect or nonsensical information but presents it confidently. RAG mitigates this by grounding the model's responses in retrieved facts."
  },
  {
    "id": "rag_3",
    "topic": "GenAI",
    "content": "Vector Databases are specialized databases designed to handle high-dimensional vector data (embeddings). They enable efficient semantic search, which is critical for the retrieval step in RAG."
  },
  {
    "id": "rag_4",
    "topic": "GenAI",
    "content": "Embeddings are numerical representations of text where words or sentences with similar meanings have similar vector representations. This allows computers to understand the semantic relationship between words."
  },
  {
    "id": "rag_5",
    "topic": "GenAI",
    "content": "Chunking is the process of breaking down large documents into smaller, manageable pieces (chunks) before embedding them. Efficient chunking ensures that the retrieved context is precise and fits within the LLM's context window."
  },
  {
    "id": "rag_6",
    "topic": "GenAI",
    "content": "The Context Window is the limit on the amount of text (tokens) an LLM can process at one time. RAG helps bypass this limitation by retrieving only the most relevant information for the current query."
  },
  {
    "id": "rag_7",
    "topic": "GenAI",
    "content": "Temperature is a hyperparameter that controls the randomness of the model's output. A low temperature (0.1) makes the output deterministic and focused, while a high temperature (0.8) makes it more random and creative."
  },
  {
    "id": "rag_8",
    "topic": "GenAI",
    "content": "Prompt Engineering involves crafting inputs (prompts) to guide Generative AI models to produce desired outputs. In RAG, the prompt is dynamically updated with retrieved context."
  },
  {
    "id": "rag_9",
    "topic": "GenAI",
    "content": "Transformers are a type of deep learning model that uses self-attention mechanisms to weigh the significance of different parts of the input data. They are the architecture behind modern LLMs like GPT and Gemini."
  },
  {
    "id": "rag_10",
    "topic": "GenAI",
    "content": "Fine-tuning involves training a pre-trained model on a specific dataset to adapt it to a specific task. RAG is often preferred over fine-tuning for ensuring factual accuracy and accessing real-time data."
  },
  {
    "id": "rag_11",
    "topic": "GenAI",
    "content": "Semantic Search searches for the meaning of the query rather than just matching keywords. It is the core retrieval mechanism in RAG systems."
  },
  {
    "id": "rag_12",
    "topic": "GenAI",
    "content": "Top-k Retrieval refers to fetching the top 'k' most similar document chunks from the vector database to use as context for the LLM."
  },
  {
    "id": "rag_13",
    "topic": "GenAI",
    "content": "Zero-shot Learning is the ability of a model to perform a task without having seen any specific examples of that task during training. LLMs often exhibit this capability."
  },
  {
    "id": "rag_14",
    "topic": "GenAI",
    "content": "Chain of Thought (CoT) prompting encourages the LLM to explain its reasoning process step-by-step, which often leads to more accurate answers for complex problems."
  },
  {
    "id": "rag_15",
    "topic": "GenAI",
    "content": "Tokenization is the process of converting text into smaller units (tokens) like words or subwords, which are the basic inputs for LLMs."
  }
]